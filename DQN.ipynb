{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xpurwar/DeepQ-Network/blob/main/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gymnasium"
      ],
      "metadata": {
        "id": "KJUp6QBRY-Qd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d42c55f6-9e2b-459f-b813-5b7ac62e01d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import sleep\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "import gymnasium as gym\n",
        "from gymnasium.envs.registration import register\n",
        "import torch\n",
        "from torch import nn\n"
      ],
      "metadata": {
        "id": "K_GLg7lbayhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device='cuda'"
      ],
      "metadata": {
        "id": "pntt2jvisFpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Give colab access to your google drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "7PWN1PkGe66q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5820a514-5716-47e1-cec7-4210340e1349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOFKVt56YOgd",
        "outputId": "47893131-c8bd-4023-db8d-75ba8451773c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Change current directory to folder with MiniPacMan\n",
        "%cd /content/drive/MyDrive/Reinforcement Learning"
      ],
      "metadata": {
        "id": "1SCX1d90YjOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48e9673e-27ac-4c46-ffdf-e1bcc1045711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Reinforcement Learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import MiniPacMan environment class definition\n",
        "from MiniPacManGymV2 import MiniPacManEnv"
      ],
      "metadata": {
        "id": "GCa5TYdVWL2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "register(\n",
        "    id=\"MiniPacMan-v2\",\n",
        "    entry_point=MiniPacManEnv,\n",
        "    max_episode_steps=20\n",
        ")"
      ],
      "metadata": {
        "id": "TcY1Q97RRy6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a MiniPacMan gymnasium environment\n",
        "env = gym.make(\"MiniPacMan-v2\", render_mode=\"human\", frozen_ghost=False)"
      ],
      "metadata": {
        "id": "k7hwnC7Ob9VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(nn.Module):\n",
        "    #Define your network here\n",
        "    #Should accept inputs of shape (6,6) and return (4,)\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(36, 32)\n",
        "        self.activation1 = nn.ReLU()\n",
        "        self.linear3 = nn.Linear(32, 16)\n",
        "        self.activation3 = nn.ReLU()\n",
        "        self.linear4 = nn.Linear(16, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation1(self.linear1(x))\n",
        "        x = self.activation3(self.linear3(x))\n",
        "        x = self.linear4(x)\n",
        "        return x\n",
        "\n",
        "model = QNetwork().to(device)\n",
        "x = torch.randn(1, 36).to(device)\n",
        "#x = torch.flatten(x)\n",
        "model(x)"
      ],
      "metadata": {
        "id": "Y6irumLQsc1p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2d666dd-5e30-40e9-e897-a54a88e868cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2932, -0.3092,  0.0383, -0.2417]], device='cuda:0',\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        if len(self.buffer) >= self.capacity:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in indices])\n",
        "        return torch.stack(states).to(device), actions, torch.tensor(rewards).to(device), torch.stack(next_states).to(device), torch.tensor(dones).to(device)"
      ],
      "metadata": {
        "id": "8D31lBLpRUC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = QNetwork().to(device) #initialize a Q network\n",
        "Q_target = QNetwork().to(device)\n",
        "Q_target.load_state_dict(Q.state_dict())\n",
        "Q_optimizer = torch.optim.Adam(Q.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "6-TgasGh9Q11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set hyperparams!\n",
        "gamma=0.95\n",
        "buffer_size = 1000\n",
        "batch_size = 32\n",
        "num_episodes = 15000\n",
        "\n",
        "RB=ReplayBuffer(buffer_size) #initialize Replay Buffer\n",
        "epsilon=1 #initialize epsilon\n",
        "\n",
        "for e in range(num_episodes):\n",
        "  new_obs,info=env.reset()\n",
        "  new_obs=torch.tensor(new_obs,dtype=torch.float32).to(device)\n",
        "\n",
        "  done=False\n",
        "  truncated=False\n",
        "  steps=0\n",
        "\n",
        "  while not done and not truncated: #Loop for one episode\n",
        "    obs = new_obs\n",
        "    #choose action\n",
        "    t = np.random.random()\n",
        "    obs = torch.flatten(obs)\n",
        "    if t > epsilon:\n",
        "      with torch.no_grad():\n",
        "        pred = Q(obs)\n",
        "        action = torch.argmax(pred)\n",
        "    else:\n",
        "      action=torch.randint(4,(1,)).item()\n",
        "\n",
        "    #take a step:\n",
        "    new_obs,reward, done, truncated, info=env.step(action)\n",
        "    new_obs=torch.tensor(new_obs,dtype=torch.float32).to(device)\n",
        "    # Add (s, a, r, s0, done) to D\n",
        "    new_obs = torch.flatten(new_obs)\n",
        "    RB.push(obs,action,reward,new_obs,done)\n",
        "    steps+=1\n",
        "\n",
        "    if len(RB.buffer) >= batch_size:\n",
        "      states, actions, rewards, next_states, dones = RB.sample(batch_size)\n",
        "      pred = Q(states)[torch.arange(batch_size), actions]\n",
        "      with torch.no_grad():\n",
        "        pred_1 = Q_target(next_states)\n",
        "        best_action = torch.max(pred_1, axis = 1).values\n",
        "        targets = rewards + (~dones) * gamma * best_action.to(device)\n",
        "      #print(pred.shape, targets.shape)\n",
        "      loss = torch.mean((pred - targets)**2)\n",
        "      Q_optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      Q_optimizer.step()\n",
        "\n",
        "  if e%100:\n",
        "    Q_target.load_state_dict(Q.state_dict())\n",
        "\n",
        "\n",
        "  #reduce episilon if its not too low:\n",
        "  min_e = 0.01\n",
        "  if epsilon > min_e:\n",
        "    epsilon = epsilon - (1/num_episodes)\n",
        "\n",
        "  #periodic reporting:\n",
        "  if e>0 and e%100==0:\n",
        "    print(f'episode: {e}, steps: {steps}, epislon: {epsilon},win: {reward==20}')\n"
      ],
      "metadata": {
        "id": "0fe-YvvwKpAZ",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f63ac9b4-fee7-4265-8519-d15fa7b70d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 100, steps: 1, epislon: 0.9932666666666674,win: False\n",
            "episode: 200, steps: 3, epislon: 0.9866000000000015,win: False\n",
            "episode: 300, steps: 1, epislon: 0.9799333333333355,win: False\n",
            "episode: 400, steps: 1, epislon: 0.9732666666666696,win: False\n",
            "episode: 500, steps: 1, epislon: 0.9666000000000037,win: False\n",
            "episode: 600, steps: 3, epislon: 0.9599333333333377,win: False\n",
            "episode: 700, steps: 1, epislon: 0.9532666666666718,win: False\n",
            "episode: 800, steps: 5, epislon: 0.9466000000000059,win: False\n",
            "episode: 900, steps: 1, epislon: 0.93993333333334,win: False\n",
            "episode: 1000, steps: 3, epislon: 0.933266666666674,win: False\n",
            "episode: 1100, steps: 6, epislon: 0.9266000000000081,win: False\n",
            "episode: 1200, steps: 3, epislon: 0.9199333333333422,win: False\n",
            "episode: 1300, steps: 5, epislon: 0.9132666666666762,win: False\n",
            "episode: 1400, steps: 6, epislon: 0.9066000000000103,win: False\n",
            "episode: 1500, steps: 4, epislon: 0.8999333333333444,win: False\n",
            "episode: 1600, steps: 3, epislon: 0.8932666666666784,win: False\n",
            "episode: 1700, steps: 1, epislon: 0.8866000000000125,win: False\n",
            "episode: 1800, steps: 2, epislon: 0.8799333333333466,win: False\n",
            "episode: 1900, steps: 1, epislon: 0.8732666666666806,win: False\n",
            "episode: 2000, steps: 1, epislon: 0.8666000000000147,win: False\n",
            "episode: 2100, steps: 3, epislon: 0.8599333333333488,win: False\n",
            "episode: 2200, steps: 4, epislon: 0.8532666666666828,win: False\n",
            "episode: 2300, steps: 2, epislon: 0.8466000000000169,win: False\n",
            "episode: 2400, steps: 1, epislon: 0.839933333333351,win: False\n",
            "episode: 2500, steps: 5, epislon: 0.833266666666685,win: False\n",
            "episode: 2600, steps: 1, epislon: 0.8266000000000191,win: False\n",
            "episode: 2700, steps: 3, epislon: 0.8199333333333532,win: False\n",
            "episode: 2800, steps: 1, epislon: 0.8132666666666872,win: False\n",
            "episode: 2900, steps: 3, epislon: 0.8066000000000213,win: False\n",
            "episode: 3000, steps: 5, epislon: 0.7999333333333554,win: False\n",
            "episode: 3100, steps: 5, epislon: 0.7932666666666894,win: False\n",
            "episode: 3200, steps: 4, epislon: 0.7866000000000235,win: False\n",
            "episode: 3300, steps: 1, epislon: 0.7799333333333576,win: False\n",
            "episode: 3400, steps: 2, epislon: 0.7732666666666916,win: False\n",
            "episode: 3500, steps: 1, epislon: 0.7666000000000257,win: False\n",
            "episode: 3600, steps: 2, epislon: 0.7599333333333598,win: False\n",
            "episode: 3700, steps: 3, epislon: 0.7532666666666938,win: False\n",
            "episode: 3800, steps: 1, epislon: 0.7466000000000279,win: False\n",
            "episode: 3900, steps: 3, epislon: 0.739933333333362,win: False\n",
            "episode: 4000, steps: 7, epislon: 0.733266666666696,win: False\n",
            "episode: 4100, steps: 4, epislon: 0.7266000000000301,win: False\n",
            "episode: 4200, steps: 1, epislon: 0.7199333333333642,win: False\n",
            "episode: 4300, steps: 3, epislon: 0.7132666666666982,win: False\n",
            "episode: 4400, steps: 4, epislon: 0.7066000000000323,win: False\n",
            "episode: 4500, steps: 1, epislon: 0.6999333333333664,win: False\n",
            "episode: 4600, steps: 2, epislon: 0.6932666666667004,win: False\n",
            "episode: 4700, steps: 1, epislon: 0.6866000000000345,win: False\n",
            "episode: 4800, steps: 1, epislon: 0.6799333333333686,win: False\n",
            "episode: 4900, steps: 2, epislon: 0.6732666666667027,win: False\n",
            "episode: 5000, steps: 6, epislon: 0.6666000000000367,win: False\n",
            "episode: 5100, steps: 3, epislon: 0.6599333333333708,win: False\n",
            "episode: 5200, steps: 2, epislon: 0.6532666666667049,win: False\n",
            "episode: 5300, steps: 1, epislon: 0.6466000000000389,win: False\n",
            "episode: 5400, steps: 1, epislon: 0.639933333333373,win: False\n",
            "episode: 5500, steps: 2, epislon: 0.6332666666667071,win: False\n",
            "episode: 5600, steps: 4, epislon: 0.6266000000000411,win: False\n",
            "episode: 5700, steps: 1, epislon: 0.6199333333333752,win: False\n",
            "episode: 5800, steps: 4, epislon: 0.6132666666667093,win: False\n",
            "episode: 5900, steps: 2, epislon: 0.6066000000000433,win: False\n",
            "episode: 6000, steps: 2, epislon: 0.5999333333333774,win: False\n",
            "episode: 6100, steps: 11, epislon: 0.5932666666667115,win: False\n",
            "episode: 6200, steps: 1, epislon: 0.5866000000000455,win: False\n",
            "episode: 6300, steps: 1, epislon: 0.5799333333333796,win: False\n",
            "episode: 6400, steps: 2, epislon: 0.5732666666667137,win: False\n",
            "episode: 6500, steps: 4, epislon: 0.5666000000000477,win: False\n",
            "episode: 6600, steps: 4, epislon: 0.5599333333333818,win: False\n",
            "episode: 6700, steps: 1, epislon: 0.5532666666667159,win: False\n",
            "episode: 6800, steps: 2, epislon: 0.5466000000000499,win: False\n",
            "episode: 6900, steps: 1, epislon: 0.539933333333384,win: False\n",
            "episode: 7000, steps: 1, epislon: 0.5332666666667181,win: False\n",
            "episode: 7100, steps: 2, epislon: 0.5266000000000521,win: False\n",
            "episode: 7200, steps: 1, epislon: 0.5199333333333862,win: False\n",
            "episode: 7300, steps: 5, epislon: 0.5132666666667203,win: False\n",
            "episode: 7400, steps: 9, epislon: 0.5066000000000543,win: False\n",
            "episode: 7500, steps: 5, epislon: 0.4999333333333884,win: False\n",
            "episode: 7600, steps: 4, epislon: 0.4932666666667225,win: False\n",
            "episode: 7700, steps: 1, epislon: 0.48660000000005654,win: False\n",
            "episode: 7800, steps: 1, epislon: 0.4799333333333906,win: False\n",
            "episode: 7900, steps: 4, epislon: 0.4732666666667247,win: False\n",
            "episode: 8000, steps: 11, epislon: 0.46660000000005875,win: False\n",
            "episode: 8100, steps: 5, epislon: 0.4599333333333928,win: False\n",
            "episode: 8200, steps: 1, epislon: 0.4532666666667269,win: False\n",
            "episode: 8300, steps: 4, epislon: 0.44660000000006095,win: False\n",
            "episode: 8400, steps: 1, epislon: 0.439933333333395,win: False\n",
            "episode: 8500, steps: 3, epislon: 0.4332666666667291,win: False\n",
            "episode: 8600, steps: 14, epislon: 0.42660000000006315,win: False\n",
            "episode: 8700, steps: 9, epislon: 0.4199333333333972,win: False\n",
            "episode: 8800, steps: 1, epislon: 0.4132666666667313,win: False\n",
            "episode: 8900, steps: 11, epislon: 0.40660000000006535,win: False\n",
            "episode: 9000, steps: 4, epislon: 0.3999333333333994,win: False\n",
            "episode: 9100, steps: 3, epislon: 0.3932666666667335,win: False\n",
            "episode: 9200, steps: 6, epislon: 0.38660000000006756,win: False\n",
            "episode: 9300, steps: 5, epislon: 0.3799333333334016,win: False\n",
            "episode: 9400, steps: 1, epislon: 0.3732666666667357,win: False\n",
            "episode: 9500, steps: 1, epislon: 0.36660000000006976,win: False\n",
            "episode: 9600, steps: 20, epislon: 0.3599333333334038,win: False\n",
            "episode: 9700, steps: 17, epislon: 0.3532666666667379,win: False\n",
            "episode: 9800, steps: 1, epislon: 0.34660000000007196,win: False\n",
            "episode: 9900, steps: 17, epislon: 0.33993333333340603,win: False\n",
            "episode: 10000, steps: 9, epislon: 0.3332666666667401,win: False\n",
            "episode: 10100, steps: 3, epislon: 0.32660000000007416,win: False\n",
            "episode: 10200, steps: 1, epislon: 0.31993333333340823,win: False\n",
            "episode: 10300, steps: 2, epislon: 0.3132666666667423,win: False\n",
            "episode: 10400, steps: 2, epislon: 0.30660000000007637,win: False\n",
            "episode: 10500, steps: 2, epislon: 0.29993333333341043,win: False\n",
            "episode: 10600, steps: 2, epislon: 0.2932666666667445,win: False\n",
            "episode: 10700, steps: 10, epislon: 0.28660000000007857,win: True\n",
            "episode: 10800, steps: 7, epislon: 0.27993333333341264,win: False\n",
            "episode: 10900, steps: 10, epislon: 0.2732666666667467,win: True\n",
            "episode: 11000, steps: 13, epislon: 0.2666000000000808,win: True\n",
            "episode: 11100, steps: 2, epislon: 0.25993333333341484,win: False\n",
            "episode: 11200, steps: 5, epislon: 0.2532666666667489,win: False\n",
            "episode: 11300, steps: 8, epislon: 0.24660000000008298,win: False\n",
            "episode: 11400, steps: 7, epislon: 0.23993333333341704,win: False\n",
            "episode: 11500, steps: 5, epislon: 0.2332666666667511,win: False\n",
            "episode: 11600, steps: 20, epislon: 0.22660000000008518,win: False\n",
            "episode: 11700, steps: 11, epislon: 0.21993333333341925,win: False\n",
            "episode: 11800, steps: 11, epislon: 0.2132666666667533,win: True\n",
            "episode: 11900, steps: 5, epislon: 0.20660000000008738,win: False\n",
            "episode: 12000, steps: 5, epislon: 0.19993333333342145,win: False\n",
            "episode: 12100, steps: 15, epislon: 0.19326666666675552,win: True\n",
            "episode: 12200, steps: 2, epislon: 0.18660000000008958,win: False\n",
            "episode: 12300, steps: 2, epislon: 0.17993333333342365,win: False\n",
            "episode: 12400, steps: 17, epislon: 0.17326666666675772,win: True\n",
            "episode: 12500, steps: 20, epislon: 0.16660000000009179,win: False\n",
            "episode: 12600, steps: 11, epislon: 0.15993333333342585,win: True\n",
            "episode: 12700, steps: 7, epislon: 0.15326666666675992,win: False\n",
            "episode: 12800, steps: 20, epislon: 0.146600000000094,win: False\n",
            "episode: 12900, steps: 12, epislon: 0.13993333333342806,win: False\n",
            "episode: 13000, steps: 17, epislon: 0.13326666666676212,win: True\n",
            "episode: 13100, steps: 8, epislon: 0.1266000000000962,win: False\n",
            "episode: 13200, steps: 2, epislon: 0.1199333333334292,win: False\n",
            "episode: 13300, steps: 1, epislon: 0.11326666666676188,win: False\n",
            "episode: 13400, steps: 7, epislon: 0.10660000000009456,win: False\n",
            "episode: 13500, steps: 14, epislon: 0.09993333333342724,win: True\n",
            "episode: 13600, steps: 16, epislon: 0.09326666666675992,win: True\n",
            "episode: 13700, steps: 15, epislon: 0.0866000000000926,win: True\n",
            "episode: 13800, steps: 11, epislon: 0.07993333333342528,win: True\n",
            "episode: 13900, steps: 1, epislon: 0.07326666666675796,win: False\n",
            "episode: 14000, steps: 11, epislon: 0.06660000000009064,win: True\n",
            "episode: 14100, steps: 11, epislon: 0.05993333333342359,win: True\n",
            "episode: 14200, steps: 13, epislon: 0.053266666666756966,win: True\n",
            "episode: 14300, steps: 15, epislon: 0.04660000000009034,win: True\n",
            "episode: 14400, steps: 9, epislon: 0.039933333333423714,win: False\n",
            "episode: 14500, steps: 10, epislon: 0.03326666666675709,win: True\n",
            "episode: 14600, steps: 10, epislon: 0.02660000000009046,win: True\n",
            "episode: 14700, steps: 17, epislon: 0.019933333333423835,win: True\n",
            "episode: 14800, steps: 11, epislon: 0.013266666666757208,win: True\n",
            "episode: 14900, steps: 10, epislon: 0.009933333333423895,win: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs, info = env.reset()\n",
        "done = False\n",
        "truncated = False\n",
        "\n",
        "while not done and not truncated:\n",
        "    env.render()\n",
        "    obs = torch.tensor(obs,dtype=torch.float32)\n",
        "    obs = torch.flatten(obs).to(device)\n",
        "    pred = Q(obs)\n",
        "    action = torch.argmax(pred)\n",
        "    obs, reward, done, truncated, info = env.step(action)\n",
        "    sleep(1)\n",
        "    clear_output(wait=True)\n",
        "\n",
        "env.render()\n",
        "env.close()"
      ],
      "metadata": {
        "id": "0SXyI97eNx6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a0e075-561c-4f62-acf1-ab17e4d7c8f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xxxxxx\n",
            "x····x\n",
            "x····x\n",
            "xᗧ···x\n",
            "x····x\n",
            "xxxxxx\n",
            "\n"
          ]
        }
      ]
    }
  ]
}